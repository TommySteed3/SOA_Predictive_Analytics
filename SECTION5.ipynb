{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE GENERATION AND SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIABLES VS FEATURES\n",
    "* Variables are the raw data from your dataset.\n",
    "* Features are what ultimately gets fed into your model.\n",
    "* You may use variables as features, or transform your variables.  Usually a combination of both.\n",
    "\n",
    "# REASONS FOR FEATURE GENERATION\n",
    "* Reduce dimensionality.\n",
    "* Help the model capture complex signals.\n",
    "\n",
    "# METHODS OF FEATURE GENERATION\n",
    "* Domain Knowledge: Combine variables, grouping, etc.\n",
    "* Filter Based Methods: Use statistical measures to determine which features are more predictive of the target.\n",
    "* Algorithmic Methods: Clustering and PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION\n",
    "* Once you have generated your features - you begin the feature selection process.\n",
    "* Define Feature Selection as the process of selecting a subset of features to train the model with.\n",
    "* There are 3 general approaches (Can use any combination of some or all):\n",
    "\n",
    "**Subject Matter Expertise:**\n",
    "* Use domain knowledge to remove nonsensical features as well as features that will be highly correlated.\n",
    "\n",
    "**Filter Based Techniques:**\n",
    "* Uses statistical tests to determine a subset of features with high predicive power.\n",
    "\n",
    "**Algorithmic Methods:**\n",
    "* Involve feature selection as part of the model itself (Embedded Methods).\n",
    "* Analyze the structure of a fitted model to understand the most important features.\n",
    "\n",
    "**Challenge:**\n",
    "* Unlimited combination of features to choose from.\n",
    "* Performance of features depends on the model and combination of variables.\n",
    "* Typically several iterations before settling on final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTER BASED TECHNIQUES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CORRELATION**  \n",
    "* **Pearson:** $r = \\rho_{x,y} = COV(X,Y)/\\sigma_x \\sigma_y $\n",
    "* Measures strength of LINEAR relationship between variables.\n",
    "* **Kendall:**  Kendall's Rank ; Kendall's Tau. Measures relationship between rankings of ordinal variables.\n",
    "* Tests how one variable increases/decreases as the other variable does.  Relationship does not have to be linear.\n",
    "* **Spearman:** Like Kendall - but magnitude is considered.\n",
    "* Kendall and Spearman's are designed for non-parametric and non-normal data (unlike Pearson's).  \n",
    "\n",
    "* Outliers impact Pearson more than Kendall and Spearman.\n",
    "\n",
    "Rule of Thumb:  Correlation value of 50% or more wih the target variable suggests a possible predictive relationship.  Can also be used as a relative value when initially filtering your feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate (Pearson) correlation with Target Variable for all Numeric Predictor Variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>price</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>pred</dt>\n",
       "\t\t<dd>0.922320691063394</dd>\n",
       "\t<dt>carat</dt>\n",
       "\t\t<dd>0.921591301193477</dd>\n",
       "\t<dt>x</dt>\n",
       "\t\t<dd>0.884435161016127</dd>\n",
       "\t<dt>y</dt>\n",
       "\t\t<dd>0.865420897864187</dd>\n",
       "\t<dt>z</dt>\n",
       "\t\t<dd>0.861249443851448</dd>\n",
       "\t<dt>table</dt>\n",
       "\t\t<dd>0.127133902121742</dd>\n",
       "\t<dt>depth</dt>\n",
       "\t\t<dd>-0.010647404584143</dd>\n",
       "\t<dt>target</dt>\n",
       "\t\t<dd>-0.0971753848620544</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[price] 1\n",
       "\\item[pred] 0.922320691063394\n",
       "\\item[carat] 0.921591301193477\n",
       "\\item[x] 0.884435161016127\n",
       "\\item[y] 0.865420897864187\n",
       "\\item[z] 0.861249443851448\n",
       "\\item[table] 0.127133902121742\n",
       "\\item[depth] -0.010647404584143\n",
       "\\item[target] -0.0971753848620544\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "price\n",
       ":   1pred\n",
       ":   0.922320691063394carat\n",
       ":   0.921591301193477x\n",
       ":   0.884435161016127y\n",
       ":   0.865420897864187z\n",
       ":   0.861249443851448table\n",
       ":   0.127133902121742depth\n",
       ":   -0.010647404584143target\n",
       ":   -0.0971753848620544\n",
       "\n"
      ],
      "text/plain": [
       "      price        pred       carat           x           y           z \n",
       " 1.00000000  0.92232069  0.92159130  0.88443516  0.86542090  0.86124944 \n",
       "      table       depth      target \n",
       " 0.12713390 -0.01064740 -0.09717538 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.diamonds <- diamonds %>%\n",
    "    select(-c(cut,color,clarity))\n",
    "\n",
    "b.Pearson <- cor(data.diamonds, method = \"pearson\")[ ,c(\"price\")]\n",
    "b.Pearson <- sort(b.Pearson, decreasing = TRUE)\n",
    "b.Pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MUTUAL INFORMATION**  \n",
    "* Measures the contribution of a variable toward reducing uncertainty about the value of another variable.\n",
    "* Useful in feature selection: maximizes mutual info between joint distribution of training variables and target in datasets with many dimensions.\n",
    "* Many variations of the mutual information score.  \n",
    "* **Entropy:**$=\\sum_{x,y} p(x,y)log\\frac{p(x,y)}{p(x)p(y)}$\n",
    "\n",
    "* Values are for relative purposes - not useful in an absolute sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MINIMUM REDUNDANCY MAXIMUM RELEVANCE METHOD**\n",
    "* Select variables that have high correlation with the target and low mutual info with other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chi Square Test**\n",
    "* $X^2=\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\frac{(O_{ij}-E_{ij})}{E_{i,j}}$ \n",
    "* Test for independence.\n",
    "* Primarily used for Classification Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITHMIC METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization**\n",
    "* Add a penalty term to your objective function (Goodness of Fit measure you are trying to minimize).  Penalty for more parameters.\n",
    "* Want to avoid overly complex models that don't generalize well.\n",
    "* Begin by covering 3 types of regularized regression.\n",
    "* Note: Important to standardize data when using regularized regression.\n",
    "\n",
    "**Ridge Regression**\n",
    "* $\\sum_{m=1}^{M} \\beta^2_m$\n",
    "* Will not reduce coefficients to zero.\n",
    "\n",
    "**Lasso Regression**\n",
    "* $\\sum_{m=1}^{M} |\\beta_m|$\n",
    "* Can be used for feature selection (can reduce coefficients to zero).\n",
    "\n",
    "**Elastic Net Regression**\n",
    "* $\\sum_{m=1}^{M} \\beta^2_m$ + $\\alpha\\sum_{m=1}^{M} |\\beta_m|$\n",
    "* Alpha: (0 < alpha < 1) mixing coefficient - determines relative weight of ridge and lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**\n",
    "* Alpha and Lambda from regularized regression are examples of hyperparameters.  Parameters that are part of the algorithm used to generate the final model.  \n",
    "* Hyperparameter tuning is the process of optimizing the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREE BASED MODELS AND VARIABLE IMPORTANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BRIEF SUMMARY OF TREE BASED MODELS**\n",
    "* Random forest models all include decision tree variants.\n",
    "* Decision trees identify optimal data splits - based on definition of optimal.\n",
    "* Splits continue until threshold reached and no more information can be gained.\n",
    "* Allows us to investigate non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VARIABLE IMPORTANCE**\n",
    "* Easy to run a random forest. \n",
    "* Inspect variable importance plots for insights into potential features in your GLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN AND VALIDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Concept:  Before submitting a model for approval - you want to see how well it performs on data that was not used in model development.  The process of developing your model is referred to as training your model.  The process of checking model performance with unseen data is called validating your model.  One of the following two approaches are generally utilized:\n",
    "* **Test/Train/Validate:** Randomly split data into 3 portions.\n",
    "    * Training dataset:  Used to train your model.  \n",
    "    * Validation dataset: Used to Validate your model.\n",
    "    * Test dataset: Portion of the data that is put away before modeling begins.  This dataset is not to be viewed again until you have finalized your model.\n",
    "    * A common split is 70/20/10 - but there is no fixed rule.  Depends on size of dataset.  Ideally you want as robust a dataset as possible for training.  Need all 3 datasets to be robust enough to give meaningful results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using caret package to create data splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "53940"
      ],
      "text/latex": [
       "53940"
      ],
      "text/markdown": [
       "53940"
      ],
      "text/plain": [
       "[1] 53940"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "26970"
      ],
      "text/latex": [
       "26970"
      ],
      "text/markdown": [
       "26970"
      ],
      "text/plain": [
       "[1] 26970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "26970"
      ],
      "text/latex": [
       "26970"
      ],
      "text/markdown": [
       "26970"
      ],
      "text/plain": [
       "[1] 26970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(caret)\n",
    "library(ggplot2)\n",
    "\n",
    "diamonds$target<- ifelse(diamonds$cut == \"Ideal\", 1,0)\n",
    "\n",
    "set.seed(1000)\n",
    "training.indices <- createDataPartition(diamonds$target, p = 0.5, list = FALSE)\n",
    "data.training <- diamonds[training.indices, ] \n",
    "data.validation <- diamonds[-training.indices, ]\n",
    "\n",
    "nrow(diamonds)\n",
    "nrow(data.training)\n",
    "nrow(data.validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **K-Fold Cross Validation:**\n",
    "    1. Set Testing dataset aside as with Test/Train/Validate approach.\n",
    "    2. Split remaining data into k equal sized splits (folds).\n",
    "    3. Train on (k-1) folds and Test on remaining fold (do this k times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING A GLM USING THE GLMNET LIBRARY\n",
    "* Use the 2 variables with highest correlation in Correlation section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAAAP8zMzNNTU1o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\n/we7UgjzbUXwQgAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(glmnet)\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "\n",
    "diamonds$variable2 <- diamonds$x\n",
    "\n",
    "X <- diamonds %>%\n",
    "        select(carat, variable2)\n",
    "\n",
    "\n",
    "X <-as.matrix(X)\n",
    "\n",
    "formula.lm <- as.formula('y ~ carat + variable2')\n",
    "model.lm <- glmnet(X,y = diamonds$price, family = 'gaussian', alpha = 0, lambda = 0)\n",
    "\n",
    "diamonds$pred <- predict(model.lm, newx = X)\n",
    "\n",
    "# Plot the results\n",
    "p1 <- ggplot(data = diamonds, aes(x = x, y = y)) + geom_point(color = \"blue\", size = 3) + geom_line(aes(y=diamonds$pred))\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View Coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2 x 1 sparse Matrix of class \"dgCMatrix\"\n",
       "                 s0\n",
       "carat     10077.253\n",
       "variable2 -1006.776"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.lm$beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIDGE REGRESSION WITH GLMNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```{r}\n",
    "library(glmnet)\n",
    "\n",
    "#Here is a clever way to create the data matrix. This one doesn't require keeping track of which columns contain the features.\n",
    "X <- model.matrix(formula.lm, data = df)\n",
    "\n",
    "#Lambda has arbitrarily been set to 0.1. Alpha = 0 implies ridge regression (1 implies lasso and anything between is elasticnet). Also, note that the default is to standardize the features, but the estimated coefficients are on the scale and location of the original values.\n",
    "model.lm.ridge <- glmnet(X, y = df$y,\n",
    "                         family = \"gaussian\",\n",
    "                         alpha = 0,\n",
    "                         lambda = 0.1)\n",
    "\n",
    "# Predict results (so we can plot the line)\n",
    "df$pred_ridge01 <- predict(model.lm.ridge, newx = X)\n",
    "\n",
    "# Plot the results\n",
    "p1 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = \"blue\", size = 3) + geom_line(aes(y=df$pred_ridge01))\n",
    "p1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
